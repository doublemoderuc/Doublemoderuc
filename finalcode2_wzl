from bs4 import BeautifulSoup
import requests
import time 
import pandas as pd
import re 

headers = {
'Cookie': 'optimizelyEndUserId=oeu1590569202932r0.8946896902430339; _ga=GA1.3.146798051.1590569208; amplitude_id_408774472b1245a7df5814f20e7484d0ruc.edu.cn=eyJkZXZpY2VJZCI6IjFlMDc5MGEwLTU0ODAtNGZlZS05YzZmLTE4MmNhNTRiNGUwNyIsInVzZXJJZCI6bnVsbCwib3B0T3V0IjpmYWxzZSwic2Vzc2lvbklkIjoxNTkwODI4OTM2NDE5LCJsYXN0RXZlbnRUaW1lIjoxNTkwODI4OTk0OTg2LCJldmVudElkIjowLCJpZGVudGlmeUlkIjoxNCwic2VxdWVuY2VOdW1iZXIiOjE0fQ==; UM_distinctid=173802d4ae3165-0736d5966079da-b7a1334-100200-173802d4ae59e3; _hjid=87eb4f47-6967-439b-aeec-2f855fa434c4; _hjTLDTest=1; CWJSESSIONID=6A2E2902A55F246BD5DD1ED6EC8D9816; cwsid=7547a64036ca4999; _sp_ses.58cf=*; _sp_id.58cf=48248723-a40c-4ff8-97db-39100ccb3c6e.1597288425.40.1600155222.1600143853.7bea23cb-0c91-4850-8ac0-e5ded1a8e3e6',
'Host': 'fhya8ff4798d0cae4ed0b725f57cfc9334c1so56of96wu9xf6ppp.fbzz.libproxy.ruc.edu.cn',
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'
}

# 创建一个空数据框进行数据存储
infos = pd.DataFrame(columns = ['title', 'publisher', 'published', 'authors', 'cited_num', 'times_cited', 'abstract', 'keywords', 'keyword_plus', 'authors_address', 'author_universtiy', 'cited','DOI'])  

# 抓取单篇文章的标题等信息
def get_infos(url):
    global infos  # 设为全局变量，否则会报错
    wb_data = requests.get(url, headers = headers)
    time.sleep(4)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    
    title = soup.select('div.title > value')         
    title = str(title).replace('[<value>', '')
    title = title.replace('</value>]', '') 
    
    publisher = soup.find('span', class_ = 'hitHilite')

    published = soup.select('p.FR_field > value')[4]
    published = str(published).replace('<value>', '')
    published = published.replace('</value>', '')
   
    if published:                               #如果published存在，则不变
        published=published
    else: 
        published=''#如果不存在，则不打印任何信息

    authors = soup.find("p", class_ = "FR_field").get_text(strip = True).replace('By:','')   

    DOI=soup.find_all('p',class_="FR_field")[4].value.text

    abstracts=soup.find_all('p',class_="FR_field")[10].text

    keywords=soup.find_all('p',class_="FR_field")[11].text.strip().replace('Author Keywords:','')

    keyword_plus=soup.find_all('p',class_="FR_field")[12].text.strip().replace('KeyWords Plus:','')

    address = soup.select('td.fr_address_row2 > a')
    authors_address = []
    for addr in address:
        authors_address.append(addr.get_text().replace('\u200e ', ''))

    cited_num = soup.find_all('span', class_ = "large-number")[1].text

    times_cited = soup.find_all('span', class_ = "large-number")[0].text
    
    university = soup.find_all('preferred_org')
    author_universtiy = []    

    for univer in university:
        author_universtiy.append(univer.get_text())

    #找网页url
    import re 
    a=url.split("?")
    d=[]
    for b in a:
        c=re.split('[/&]',b)
        d.append(c)
    d[1][-1]=d[1][-1].replace('doc=','')#给定parentDoc的
    j=str(int(d[1][-1])+1)
    url_reference=[]
    #生成reference的url
    rurl='http://'+str(d[0][2])
    dproduct=str(d[1][0])
    url_r=rurl+'/summary.do?'+dproduct+'&parentProduct=UA&searchmode=CitedRefList&parentQid=1&parentDoc='+d[1][-1]+'&qid='+j+'&'+d[1][3]+'&colName=WOS&'+d[1][4]
    url_reference.append(url_r)
    
    #测试是否存在参考文献的下一页

    page_len=int(int(cited_num)/30)+1
    if page_len>1:
        for i in range(2,page_len+1):
            next_page_url=rurl+'/summary.do?'+dproduct+'&parentProduct=UA&searchmode=CitedRefList&parentQid='+d[1][-1]+'&qid='+j+'&'+d[1][3]+'&colName=WOS&page='+str(i)
            url_reference.append(next_page_url)
            
    #进行url_reference的爬取
    cited=[]
    for url_r in url_reference:
        wb_data = requests.get(url_r, headers = headers)
        soup = BeautifulSoup(wb_data.text, 'lxml')
        #爬取文献的代码
        cites=soup.find_all('div',class_='reference-item-non-ar')
        for cite in cites:
            cite = "".join(cite.text.split())#cite.text是用来获取文本的，split将空格等分开，再用join连接起来
            cite=cite.replace('\u200f',' ')
            cited.append(cite)    
  
    info = pd.DataFrame({"title": [title], "publisher":[publisher.get_text()], "published":[published], "authors": [authors], "cited_num":[cited_num], "times_cited":[times_cited],  "abstract": [abstracts], "keywords":[keywords], "keyword_plus":[keyword_plus], "authors_address":[authors_address], "cited":[cited],"DOI":[DOI],"author_universtiy":[author_universtiy]})
    infos = infos.append(info, sort = False)  # 向infos数据框中添加行
    return infos
# 待爬网址列表
urls=['http://fhya8ff4798d0cae4ed0b725f57cfc9334c1sf5cq6fw6fquu6x95.fbzz.libproxy.ruc.edu.cn/full_record.do?product=UA&search_mode=AdvancedSearch&qid=1&SID=8BgLStndaDqY8W28NzB&page=1&doc={}'.format(str(i)) for i in range(66,76)]

# 循环爬取数据
for url in urls:
     get_infos(url)
infos  # 结果列表

# 保存为csv文件
infos.to_csv("JRSB.csv", mode='a',index = False) 
